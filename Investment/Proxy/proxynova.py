import os
import time
import logging
import requests
import schedule
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright, TimeoutError
import urllib3
import warnings

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# -------------------------- 1. æ—¥å¿—é…ç½®åˆå§‹åŒ–ï¼ˆå…¨å±€ç”Ÿæ•ˆï¼‰ --------------------------
def init_logger(log_dir: str = "./proxy_logs"):
    """
    åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’ŒæŒ‰æ—¥å‘½åçš„æ—¥å¿—æ–‡ä»¶ï¼Œæ ¼å¼åŒ…å«æ—¶é—´ã€çº§åˆ«ã€æ¨¡å—ã€æ¶ˆæ¯

    Args:
        log_dir: æ—¥å¿—æ–‡ä»¶ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ä¸‹proxy_logsæ–‡ä»¶å¤¹ï¼‰
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # æ—¥å¿—æ–‡ä»¶å‘½åï¼ˆæŒ‰æ—¥æœŸï¼Œå¦‚ï¼šproxy_crawl_20251030.logï¼‰
    today_date = datetime.now().strftime("%Y%m%d")
    log_file = os.path.join(log_dir, f"proxy_crawl_{today_date}_proxynova.log")

    # æ—¥å¿—æ ¼å¼é…ç½®
    log_format = "%(asctime)s - %(levelname)s - %(module)s:%(funcName)s - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"

    # 1. é…ç½®æ§åˆ¶å° handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))

    # 2. é…ç½®æ–‡ä»¶ handlerï¼ˆUTF-8ç¼–ç ï¼Œé¿å…ä¸­æ–‡ä¹±ç ï¼‰
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)  # æ–‡ä»¶æ—¥å¿—è®°å½•DEBUGåŠä»¥ä¸Šçº§åˆ«ï¼ˆæ›´è¯¦ç»†ï¼‰
    file_handler.setFormatter(logging.Formatter(log_format, date_format))

    # 3. å…¨å±€æ—¥å¿—å™¨é…ç½®
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)  # å…¨å±€æ—¥å¿—çº§åˆ«ï¼ˆéœ€ä½äºå„handlerçº§åˆ«ï¼‰
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    # é¿å…æ—¥å¿—é‡å¤è¾“å‡ºï¼ˆç§»é™¤é»˜è®¤handlerï¼‰
    if root_logger.handlers:
        root_logger.handlers = root_logger.handlers[-2:]  # åªä¿ç•™æ§åˆ¶å°å’Œæ–‡ä»¶handler

    logging.info("âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼š%s", log_file)


# -------------------------- 2. ä»£ç†æœ‰æ•ˆæ€§éªŒè¯ï¼ˆå¤šçº¿ç¨‹å¹¶å‘ï¼‰ --------------------------
def verify_single_proxy(
        ip: str,
        port: int,
        timeout: float = 5.0,
        test_urls: dict = None
) -> bool:
    """
    éªŒè¯å•ä¸ªä»£ç†çš„æœ‰æ•ˆæ€§ï¼šæµ‹è¯•HTTP/HTTPSè¿é€šæ€§ï¼Œè¿”å›æ˜¯å¦å¯ç”¨

    Args:
        ip: ä»£ç†IPåœ°å€
        port: ä»£ç†ç«¯å£
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤5ç§’ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡ï¼‰
        test_urls: æµ‹è¯•ç«™ç‚¹å­—å…¸ï¼ˆkey: åè®®, value: æµ‹è¯•URLï¼‰

    Returns:
        bool: ä»£ç†å¯ç”¨è¿”å›Trueï¼Œä¸å¯ç”¨è¿”å›False
    """
    # ä½¿ç”¨ç±»ä¼¼curlçš„æ–¹å¼éªŒè¯ä»£ç†
    try:
        # ä½¿ç”¨http://azenv.net/éªŒè¯ä»£ç†
        response = requests.get(
            "http://azenv.net/",
            proxies={
                "http": f"http://{ip}:{port}",
                "https": f"http://{ip}:{port}"
            },
            timeout=timeout,
            verify=False
        )
        
        # æ£€æŸ¥å“åº”å†…å®¹ä¸­æ˜¯å¦åŒ…å«ä»£ç†ä¿¡æ¯
        if response.status_code == 200:
            content = response.text
            # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«IPåœ°å€ï¼Œç¡®è®¤ä»£ç†å·¥ä½œæ­£å¸¸
            if ip in content:
                logging.debug("âœ… ä»£ç†å¯ç”¨ï¼ˆCURLæ–¹å¼ï¼‰ï¼š%s:%s", ip, port)
                return True
            else:
                logging.debug("âŒ ä»£ç†æ— æ•ˆï¼ˆå“åº”å¼‚å¸¸ï¼‰ï¼š%s:%sï¼Œå“åº”å†…å®¹ï¼š%s", ip, port, content[:50])
                return False
        else:
            logging.debug("âŒ ä»£ç†æ— æ•ˆï¼ˆçŠ¶æ€ç ï¼‰ï¼š%s:%sï¼ŒçŠ¶æ€ç ï¼š%d", ip, port, response.status_code)
            return False
    except requests.exceptions.ConnectTimeout:
        logging.debug("âŒ ä»£ç†è¶…æ—¶ï¼ˆè¿æ¥è¶…æ—¶ï¼‰ï¼š%s:%s", ip, port)
        return False
    except requests.exceptions.ProxyError:
        logging.debug("âŒ ä»£ç†é”™è¯¯ï¼ˆæ— æ³•è¿æ¥ä»£ç†ï¼‰ï¼š%s:%s", ip, port)
        return False
    except Exception as e:
        logging.warning("âŒ ä»£ç†éªŒè¯å¼‚å¸¸ï¼š%s:%sï¼Œå¼‚å¸¸ä¿¡æ¯ï¼š%s", ip, port, str(e)[:100])
        return False


def verify_proxy_batch(
        proxy_df: pd.DataFrame,
        max_workers: int = 10,
        timeout: float = 5.0
) -> pd.DataFrame:
    """
    æ‰¹é‡éªŒè¯ä»£ç†æœ‰æ•ˆæ€§ï¼ˆå¤šçº¿ç¨‹å¹¶å‘ï¼‰ï¼Œè¿”å›è¿‡æ»¤åçš„æœ‰æ•ˆä»£ç†DataFrame

    Args:
        proxy_df: å¾…éªŒè¯çš„ä»£ç†DataFrameï¼ˆéœ€åŒ…å«"Proxy IP"å’Œ"Proxy Port"åˆ—ï¼‰
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤10ï¼Œé¿å…å¹¶å‘è¿‡é«˜è¢«æµ‹è¯•ç«™ç‚¹å°ç¦ï¼‰
        timeout: å•ä¸ªä»£ç†éªŒè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        pd.DataFrame: ä»…åŒ…å«æœ‰æ•ˆä»£ç†çš„DataFrameï¼ˆç©ºåˆ™è¿”å›ç©ºDataFrameï¼‰
    """
    if proxy_df.empty:
        logging.warning("âš ï¸  å¾…éªŒè¯ä»£ç†ä¸ºç©ºï¼Œæ— éœ€éªŒè¯")
        return pd.DataFrame()

    logging.info("ğŸ“‹ å¼€å§‹æ‰¹é‡éªŒè¯ä»£ç†ï¼Œå¾…éªŒè¯æ•°é‡ï¼š%dï¼Œå¹¶å‘çº¿ç¨‹æ•°ï¼š%dï¼Œè¶…æ—¶æ—¶é—´ï¼š%ds",
                 len(proxy_df), max_workers, timeout)

    # å­˜å‚¨æœ‰æ•ˆä»£ç†çš„ç´¢å¼•
    valid_proxy_indices = []

    # å¤šçº¿ç¨‹æ‰§è¡ŒéªŒè¯
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä¸ºæ¯ä¸ªä»£ç†æäº¤éªŒè¯ä»»åŠ¡ï¼ˆè¿”å› (ä»»åŠ¡, ä»£ç†ç´¢å¼•) æ˜ å°„ï¼‰
        task_map = {}
        for idx, row in proxy_df.iterrows():
            ip = str(row["Proxy IP"]).strip()
            port = int(row["Proxy Port"])  # ç¡®ä¿ç«¯å£ä¸ºæ•´æ•°
            task = executor.submit(verify_single_proxy, ip=ip, port=port, timeout=timeout)
            task_map[task] = idx

        # éå†å®Œæˆçš„ä»»åŠ¡ï¼Œæ”¶é›†æœ‰æ•ˆä»£ç†ç´¢å¼•
        for task in as_completed(task_map):
            idx = task_map[task]
            if task.result():  # è‹¥ä»£ç†æœ‰æ•ˆï¼Œè®°å½•ç´¢å¼•
                valid_proxy_indices.append(idx)

    # è¿‡æ»¤æœ‰æ•ˆä»£ç†
    valid_proxy_df = proxy_df.loc[valid_proxy_indices].reset_index(drop=True)
    logging.info("ğŸ“Š ä»£ç†éªŒè¯å®Œæˆï¼šå¾…éªŒè¯%dæ¡ â†’ æœ‰æ•ˆ%dæ¡ â†’ æ— æ•ˆ%dæ¡",
                 len(proxy_df), len(valid_proxy_df), len(proxy_df) - len(valid_proxy_df))

    return valid_proxy_df


# -------------------------- 3. æ ¸å¿ƒçˆ¬å–å‡½æ•°ï¼ˆæ•´åˆéªŒè¯+æ—¥å¿—ï¼‰ --------------------------
def crawl_proxies(
        url: str = "https://www.proxynova.com/proxy-server-list/",  # ç›®æ ‡é¡µé¢URL
        save_dir: str = "./proxy_data",  # CSVä¿å­˜ç›®å½•
        filter_invalid: bool = True,
        verify_proxies: bool = True,  # æ˜¯å¦å¼€å¯ä»£ç†éªŒè¯
        max_workers: int = 10,
        verify_timeout: float = 5.0
) -> pd.DataFrame:
    """
    æ— å¤´æ¨¡å¼çˆ¬å–ProxyNovaå…è´¹ä»£ç† â†’ ï¼ˆå¯é€‰ï¼‰è¿‡æ»¤æ— æ•ˆIP â†’ ï¼ˆå¯é€‰ï¼‰éªŒè¯æœ‰æ•ˆæ€§ â†’ ä¿å­˜CSV â†’ è¿”å›DataFrame

    Args:
        url: ç›®æ ‡ä»£ç†é¡µé¢URL
        save_dir: CSVä¿å­˜ç›®å½•
        filter_invalid: æ˜¯å¦è¿‡æ»¤å†…ç½‘/æ— æ•ˆIPï¼ˆ0.0.0.0ã€127.0.0.1ç­‰ï¼‰
        verify_proxies: æ˜¯å¦å¼€å¯ä»£ç†æœ‰æ•ˆæ€§éªŒè¯ï¼ˆé»˜è®¤å¼€å¯ï¼‰
        max_workers: ä»£ç†éªŒè¯æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        verify_timeout: å•ä¸ªä»£ç†éªŒè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        pd.DataFrame: æœ‰æ•ˆä»£ç†DataFrameï¼ˆå¤±è´¥è¿”å›Noneï¼‰
    """
    logging.info("ğŸš€ å¼€å§‹çˆ¬å–ProxyNovaå…è´¹ä»£ç†ï¼Œç›®æ ‡URLï¼š%s", url)

    # 1. åˆå§‹åŒ–ä¿å­˜ç›®å½•
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        logging.debug("ğŸ“‚ åˆ›å»ºä»£ç†æ•°æ®ä¿å­˜ç›®å½•ï¼š%s", save_dir)

    # 2. Playwrightæ— å¤´çˆ¬å–
    browser = None
    context = None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,  # æ”¹ä¸ºæ— å¤´æ¨¡å¼
                args=["--no-sandbox", "--disable-dev-shm-usage"]  # è§£å†³Linuxæƒé™é—®é¢˜
            )
            context = browser.new_context()
            page = context.new_page()
            logging.debug("ğŸŒ å¯åŠ¨æ— å¤´Chromiumæµè§ˆå™¨ï¼Œè®¿é—®ç›®æ ‡é¡µé¢")

            # è®¿é—®é¡µé¢ï¼ˆè¶…æ—¶30ç§’ï¼‰
            page.goto(url, timeout=30000)
            page.wait_for_load_state("networkidle", timeout=20000)  # ç­‰å¾…ç½‘ç»œç©ºé—²
            logging.debug("âœ… ç›®æ ‡é¡µé¢åŠ è½½å®Œæˆ")

            # 3. æå–ä»£ç†æ•°æ®ï¼ˆä½¿ç”¨æ–°çš„æ–¹æ³•ï¼‰
            logging.debug("ğŸ“¥ å¼€å§‹æå–ä»£ç†æ•°æ®...")
            
            # 4. ä½¿ç”¨JavaScriptæå–ä»£ç†æ•°æ®
            proxy_data = page.evaluate(r"""() => {
                const rows = [];
                // è·å–è¡¨æ ¼è¡Œ
                const tableRows = document.querySelectorAll('table:nth-of-type(1) tbody tr');
                
                for (let i = 0; i < tableRows.length; i++) {
                    const row = tableRows[i];
                    const cells = row.querySelectorAll('td');
                    
                    if (cells.length >= 7) {
                        // æå–IPï¼ˆå¤„ç†éšè—çš„IPéƒ¨åˆ†å’ŒJavaScriptæ··æ·†ï¼‰
                        let ip = '';
                        const abbrElement = cells[0].querySelector('abbr[title]');
                        if (abbrElement) {
                            // ä½¿ç”¨titleå±æ€§ä¸­çš„å®Œæ•´IPåœ°å€
                            ip = abbrElement.getAttribute('title').trim();
                        } else {
                            // ä»æ–‡æœ¬å†…å®¹ä¸­æå–IPåœ°å€ï¼ˆå»é™¤JavaScriptä»£ç ï¼‰
                            const text = cells[0].textContent.trim();
                            // æŸ¥æ‰¾ç±»ä¼¼IPåœ°å€çš„æ¨¡å¼ (x.x.x.x)
                            const ipMatch = text.match(/(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})$/);
                            if (ipMatch) {
                                ip = ipMatch[1];
                            } else {
                                ip = text;
                            }
                        }
                        
                        // æå–ç«¯å£
                        const port = cells[1].textContent.trim();
                        
                        // æå–æœ€åæ£€æŸ¥æ—¶é—´
                        const lastCheck = cells[2].textContent.trim();
                        
                        // æå–ä»£ç†é€Ÿåº¦
                        const speed = cells[3].textContent.trim();
                        
                        // æå–æ­£å¸¸è¿è¡Œæ—¶é—´
                        const uptime = cells[4].textContent.trim();
                        
                        // æå–å›½å®¶ä¿¡æ¯
                        const countryElement = cells[5].querySelector('a');
                        const country = countryElement ? countryElement.textContent.trim() : cells[5].textContent.trim();
                        
                        // æå–åŒ¿åæ€§
                        const anonymity = cells[6].textContent.trim();
                        
                        if (ip && port) {
                            rows.push({
                                ip: ip,
                                port: port,
                                last_check: lastCheck,
                                speed: speed,
                                uptime: uptime,
                                country: country,
                                anonymity: anonymity
                            });
                        }
                    }
                }
                
                return rows;
            }""")
            
            logging.debug("ğŸ“¥ æå–ä»£ç†æ•°æ®å®Œæˆï¼Œå…±è·å– %d æ¡è®°å½•", len(proxy_data))

            # 4. è½¬æ¢ä¸ºDataFrame
            if not proxy_data:
                logging.error("âŒ æœªè·å–åˆ°ä»£ç†æ•°æ®")
                return None
                
            df = pd.DataFrame(proxy_data)
            
            # é‡å‘½ååˆ—ä»¥åŒ¹é…åŸæœ‰æ ¼å¼
            df = df.rename(columns={
                'ip': 'Proxy IP',
                'port': 'Proxy Port',
                'last_check': 'Last Check',
                'speed': 'Proxy Speed',
                'uptime': 'Uptime',
                'country': 'Proxy Country',
                'anonymity': 'Anonymity'
            })

            logging.info("ğŸ“Š ä»£ç†æ•°æ®è§£æå®Œæˆï¼Œå…±%dæ¡è®°å½•ï¼ˆå­—æ®µï¼š%sï¼‰",
                         len(df), ", ".join(df.columns.tolist()))

            # 5. è¿‡æ»¤å†…ç½‘/æ— æ•ˆIP
            if filter_invalid:
                invalid_ips = ["0.0.0.0", "127.0.0.1", "localhost"]
                before_filter = len(df)
                df = df[~df["Proxy IP"].isin(invalid_ips)]
                df = df[df["Proxy Port"].apply(lambda x: str(x).isdigit())]  # è¿‡æ»¤éæ•°å­—ç«¯å£
                logging.info("ğŸ” è¿‡æ»¤æ— æ•ˆIP/ç«¯å£ï¼šè¿‡æ»¤å‰%dæ¡ â†’ è¿‡æ»¤å%dæ¡", before_filter, len(df))

            # 6. ä»£ç†æœ‰æ•ˆæ€§éªŒè¯ï¼ˆå¯é€‰ï¼‰
            if verify_proxies and not df.empty:
                df = verify_proxy_batch(df, max_workers=max_workers, timeout=verify_timeout)

            # 7. ä¿å­˜CSVï¼ˆæŒ‰æ—¥æœŸå‘½åï¼‰
            today_date = datetime.now().strftime("%Y%m%d")
            save_path = os.path.join(save_dir, f"proxynova_proxies_valid_{today_date}.csv")  # æ–‡ä»¶ååŠ validåŒºåˆ†æœ‰æ•ˆä»£ç†
            df.to_csv(save_path, index=False, encoding="utf-8-sig")
            logging.info("ğŸ’¾ æœ‰æ•ˆä»£ç†æ•°æ®ä¿å­˜å®Œæˆï¼Œè·¯å¾„ï¼š%sï¼Œæœ‰æ•ˆè®°å½•æ•°ï¼š%d", save_path, len(df))

            return df

    except TimeoutError:
        logging.error("âŒ çˆ¬å–è¶…æ—¶ï¼šé¡µé¢åŠ è½½æˆ–å…ƒç´ å®šä½è¶…è¿‡30ç§’ï¼ˆç›®æ ‡URLï¼š%sï¼‰", url)
    except Exception as e:
        logging.error("âŒ çˆ¬å–å¼‚å¸¸ï¼š%s", str(e), exc_info=True)  # exc_info=Trueè®°å½•å®Œæ•´å †æ ˆä¿¡æ¯
    finally:
        # å®‰å…¨å…³é—­æµè§ˆå™¨
        try:
            if context:
                context.close()
        except:
            pass
        try:
            if browser:
                browser.close()
        except:
            pass
        logging.debug("ğŸ”Œ å…³é—­æ— å¤´Chromiumæµè§ˆå™¨")

    return None


# -------------------------- 4. å®šæ—¶ä»»åŠ¡å‡½æ•°ï¼ˆæ•´åˆæ—¥å¿—ï¼‰ --------------------------
def schedule_daily_crawl(
        url: str = "https://www.proxynova.com/proxy-server-list/",
        save_dir: str = "./proxy_data",
        crawl_time: str = "02:00",
        verify_proxies: bool = True,
        max_workers: int = 10,
        verify_timeout: float = 5.0
):
    """
    æ¯å¤©æŒ‡å®šæ—¶é—´å®šæ—¶çˆ¬å–ä»£ç†ï¼ˆæ•´åˆæ—¥å¿—è®°å½•ï¼‰

    Args:
        url: ç›®æ ‡ä»£ç†é¡µé¢URL
        save_dir: CSVä¿å­˜ç›®å½•
        crawl_time: æ¯å¤©çˆ¬å–æ—¶é—´ï¼ˆæ ¼å¼"HH:MM"ï¼‰
        verify_proxies: æ˜¯å¦å¼€å¯ä»£ç†éªŒè¯
        max_workers: éªŒè¯å¹¶å‘çº¿ç¨‹æ•°
        verify_timeout: éªŒè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    """
    # é¦–æ¬¡è¿è¡Œç«‹å³çˆ¬å–
    logging.info("ğŸ“… é¦–æ¬¡çˆ¬å–å¯åŠ¨ï¼ˆå½“å‰æ—¶é—´ï¼š%sï¼‰", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    crawl_proxies(
        url=url,
        save_dir=save_dir,
        verify_proxies=verify_proxies,
        max_workers=max_workers,
        verify_timeout=verify_timeout
    )

    # è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡
    schedule.every().day.at(crawl_time).do(
        crawl_proxies,
        url=url,
        save_dir=save_dir,
        verify_proxies=verify_proxies,
        max_workers=max_workers,
        verify_timeout=verify_timeout
    )
    logging.info("â° å®šæ—¶ä»»åŠ¡é…ç½®å®Œæˆï¼šæ¯å¤© %s è‡ªåŠ¨æ‰§è¡Œä»£ç†çˆ¬å–", crawl_time)
    logging.info("â„¹ï¸  ç¨‹åºè¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C ç»ˆæ­¢...")

    # å¾ªç¯ç›‘å¬ä»»åŠ¡ï¼ˆæ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼‰
    while True:
        schedule.run_pending()
        time.sleep(60)


# -------------------------- ç¨‹åºå…¥å£ï¼ˆåˆå§‹åŒ–æ—¥å¿—+å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼‰ --------------------------
if __name__ == "__main__":
    # -------------------------- é…ç½®å‚æ•°ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰ --------------------------
    TARGET_URL = "https://www.proxynova.com/proxy-server-list/"  # ç›®æ ‡ä»£ç†é¡µé¢çœŸå®URL
    SAVE_DIR = "./proxy_data"  # æœ‰æ•ˆä»£ç†CSVä¿å­˜ç›®å½•
    LOG_DIR = "./proxy_logs"  # æ—¥å¿—æ–‡ä»¶ä¿å­˜ç›®å½•
    DAILY_CRAWL_TIME = "02:00"  # æ¯æ—¥çˆ¬å–æ—¶é—´ï¼ˆ24å°æ—¶åˆ¶ï¼Œå¦‚"02:00"ï¼‰
    VERIFY_PROXIES = True  # æ˜¯å¦å¼€å¯ä»£ç†æœ‰æ•ˆæ€§éªŒè¯
    MAX_WORKERS = 15  # ä»£ç†éªŒè¯æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®®10-20ï¼‰
    VERIFY_TIMEOUT = 6.0  # å•ä¸ªä»£ç†éªŒè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œå»ºè®®5-10ï¼‰
    # ----------------------------------------------------------------------------------

    # 1. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆå¿…é¡»åœ¨æœ€å‰é¢æ‰§è¡Œï¼Œç¡®ä¿åç»­æµç¨‹æ—¥å¿—æ­£å¸¸è®°å½•ï¼‰
    init_logger(log_dir=LOG_DIR)
    
    # 2. çˆ¬å–ä»£ç†
    crawl_proxies(url=TARGET_URL, save_dir=SAVE_DIR, verify_proxies=VERIFY_PROXIES,
                 max_workers=MAX_WORKERS, verify_timeout=VERIFY_TIMEOUT)

    # # 3. å¯åŠ¨å®šæ—¶çˆ¬å–ä»»åŠ¡
    # try:
    #     schedule_daily_crawl(
    #         url=TARGET_URL,
    #         save_dir=SAVE_DIR,
    #         crawl_time=DAILY_CRAWL_TIME,
    #         verify_proxies=VERIFY_PROXIES,
    #         max_workers=MAX_WORKERS,
    #         verify_timeout=VERIFY_TIMEOUT
    #     )
    # except KeyboardInterrupt:
    #     logging.info("ğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢ç¨‹åºï¼ˆCtrl+Cï¼‰ï¼Œç¨‹åºé€€å‡º")
    # except Exception as e:
    #     logging.critical("ğŸ’¥ ç¨‹åºæ„å¤–ç»ˆæ­¢ï¼Œå¼‚å¸¸ä¿¡æ¯ï¼š%s", str(e), exc_info=True)