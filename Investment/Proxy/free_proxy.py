import os
import time
import logging
import requests
import schedule
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright, TimeoutError
import urllib3
import warnings

# Á¶ÅÁî®SSLË≠¶Âëä
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# -------------------------- 1. Êó•ÂøóÈÖçÁΩÆÂàùÂßãÂåñÔºàÂÖ®Â±ÄÁîüÊïàÔºâ --------------------------
def init_logger(log_dir: str = "./proxy_logs"):
    """
    ÂàùÂßãÂåñÊó•ÂøóÁ≥ªÁªüÔºöÂêåÊó∂ËæìÂá∫Âà∞ÊéßÂà∂Âè∞ÂíåÊåâÊó•ÂëΩÂêçÁöÑÊó•ÂøóÊñá‰ª∂ÔºåÊ†ºÂºèÂåÖÂê´Êó∂Èó¥„ÄÅÁ∫ßÂà´„ÄÅÊ®°Âùó„ÄÅÊ∂àÊÅØ

    Args:
        log_dir: Êó•ÂøóÊñá‰ª∂‰øùÂ≠òÁõÆÂΩïÔºàÈªòËÆ§ÂΩìÂâçÁõÆÂΩï‰∏ãproxy_logsÊñá‰ª∂Â§πÔºâ
    """
    # ÂàõÂª∫Êó•ÂøóÁõÆÂΩïÔºà‰∏çÂ≠òÂú®ÂàôÂàõÂª∫Ôºâ
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # Êó•ÂøóÊñá‰ª∂ÂëΩÂêçÔºàÊåâÊó•ÊúüÔºåÂ¶ÇÔºöproxy_crawl_20251030.logÔºâ
    today_date = datetime.now().strftime("%Y%m%d")
    log_file = os.path.join(log_dir, f"proxy_crawl_{today_date}.log")

    # Êó•ÂøóÊ†ºÂºèÈÖçÁΩÆ
    log_format = "%(asctime)s - %(levelname)s - %(module)s:%(funcName)s - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"

    # 1. ÈÖçÁΩÆÊéßÂà∂Âè∞ handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))

    # 2. ÈÖçÁΩÆÊñá‰ª∂ handlerÔºàUTF-8ÁºñÁ†ÅÔºåÈÅøÂÖç‰∏≠Êñá‰π±Á†ÅÔºâ
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)  # Êñá‰ª∂Êó•ÂøóËÆ∞ÂΩïDEBUGÂèä‰ª•‰∏äÁ∫ßÂà´ÔºàÊõ¥ËØ¶ÁªÜÔºâ
    file_handler.setFormatter(logging.Formatter(log_format, date_format))

    # 3. ÂÖ®Â±ÄÊó•ÂøóÂô®ÈÖçÁΩÆ
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)  # ÂÖ®Â±ÄÊó•ÂøóÁ∫ßÂà´ÔºàÈúÄ‰Ωé‰∫éÂêÑhandlerÁ∫ßÂà´Ôºâ
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    # ÈÅøÂÖçÊó•ÂøóÈáçÂ§çËæìÂá∫ÔºàÁßªÈô§ÈªòËÆ§handlerÔºâ
    if root_logger.handlers:
        root_logger.handlers = root_logger.handlers[-2:]  # Âè™‰øùÁïôÊéßÂà∂Âè∞ÂíåÊñá‰ª∂handler

    logging.info("‚úÖ Êó•ÂøóÁ≥ªÁªüÂàùÂßãÂåñÂÆåÊàêÔºåÊó•ÂøóÊñá‰ª∂‰øùÂ≠òË∑ØÂæÑÔºö%s", log_file)


# -------------------------- 2. ‰ª£ÁêÜÊúâÊïàÊÄßÈ™åËØÅÔºàÂ§öÁ∫øÁ®ãÂπ∂ÂèëÔºâ --------------------------
def verify_single_proxy(
        ip: str,
        port: int,
        timeout: float = 5.0,
        test_urls: dict = None
) -> bool:
    """
    È™åËØÅÂçï‰∏™‰ª£ÁêÜÁöÑÊúâÊïàÊÄßÔºöÊµãËØïHTTP/HTTPSËøûÈÄöÊÄßÔºåËøîÂõûÊòØÂê¶ÂèØÁî®

    Args:
        ip: ‰ª£ÁêÜIPÂú∞ÂùÄ
        port: ‰ª£ÁêÜÁ´ØÂè£
        timeout: Ë∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºåÈªòËÆ§5ÁßíÔºåÈÅøÂÖçÈïøÊó∂Èó¥ÈòªÂ°ûÔºâ
        test_urls: ÊµãËØïÁ´ôÁÇπÂ≠óÂÖ∏Ôºàkey: ÂçèËÆÆ, value: ÊµãËØïURLÔºâ

    Returns:
        bool: ‰ª£ÁêÜÂèØÁî®ËøîÂõûTrueÔºå‰∏çÂèØÁî®ËøîÂõûFalse
    """
    # ‰ΩøÁî®Á±ª‰ººcurlÁöÑÊñπÂºèÈ™åËØÅ‰ª£ÁêÜ
    try:
        # ‰ΩøÁî®http://azenv.net/È™åËØÅ‰ª£ÁêÜ
        response = requests.get(
            "http://azenv.net/",
            proxies={
                "http": f"http://{ip}:{port}",
                "https": f"http://{ip}:{port}"
            },
            timeout=timeout,
            verify=False
        )
        
        # Ê£ÄÊü•ÂìçÂ∫îÂÜÖÂÆπ‰∏≠ÊòØÂê¶ÂåÖÂê´‰ª£ÁêÜ‰ø°ÊÅØ
        if response.status_code == 200:
            content = response.text
            # Ê£ÄÊü•ÂìçÂ∫î‰∏≠ÊòØÂê¶ÂåÖÂê´IPÂú∞ÂùÄÔºåÁ°ÆËÆ§‰ª£ÁêÜÂ∑•‰ΩúÊ≠£Â∏∏
            if ip in content:
                logging.debug("‚úÖ ‰ª£ÁêÜÂèØÁî®ÔºàCURLÊñπÂºèÔºâÔºö%s:%s", ip, port)
                return True
            else:
                logging.debug("‚ùå ‰ª£ÁêÜÊó†ÊïàÔºàÂìçÂ∫îÂºÇÂ∏∏ÔºâÔºö%s:%sÔºåÂìçÂ∫îÂÜÖÂÆπÔºö%s", ip, port, content[:50])
                return False
        else:
            logging.debug("‚ùå ‰ª£ÁêÜÊó†ÊïàÔºàÁä∂ÊÄÅÁ†ÅÔºâÔºö%s:%sÔºåÁä∂ÊÄÅÁ†ÅÔºö%d", ip, port, response.status_code)
            return False
    except requests.exceptions.ConnectTimeout:
        logging.debug("‚ùå ‰ª£ÁêÜË∂ÖÊó∂ÔºàËøûÊé•Ë∂ÖÊó∂ÔºâÔºö%s:%s", ip, port)
        return False
    except requests.exceptions.ProxyError:
        logging.debug("‚ùå ‰ª£ÁêÜÈîôËØØÔºàÊó†Ê≥ïËøûÊé•‰ª£ÁêÜÔºâÔºö%s:%s", ip, port)
        return False
    except Exception as e:
        logging.warning("‚ùå ‰ª£ÁêÜÈ™åËØÅÂºÇÂ∏∏Ôºö%s:%sÔºåÂºÇÂ∏∏‰ø°ÊÅØÔºö%s", ip, port, str(e)[:100])
        return False


def verify_proxy_batch(
        proxy_df: pd.DataFrame,
        max_workers: int = 10,
        timeout: float = 5.0
) -> pd.DataFrame:
    """
    ÊâπÈáèÈ™åËØÅ‰ª£ÁêÜÊúâÊïàÊÄßÔºàÂ§öÁ∫øÁ®ãÂπ∂ÂèëÔºâÔºåËøîÂõûËøáÊª§ÂêéÁöÑÊúâÊïà‰ª£ÁêÜDataFrame

    Args:
        proxy_df: ÂæÖÈ™åËØÅÁöÑ‰ª£ÁêÜDataFrameÔºàÈúÄÂåÖÂê´"IP Address"Âíå"Port"ÂàóÔºâ
        max_workers: ÊúÄÂ§ßÂπ∂ÂèëÁ∫øÁ®ãÊï∞ÔºàÈªòËÆ§10ÔºåÈÅøÂÖçÂπ∂ÂèëËøáÈ´òË¢´ÊµãËØïÁ´ôÁÇπÂ∞ÅÁ¶ÅÔºâ
        timeout: Âçï‰∏™‰ª£ÁêÜÈ™åËØÅË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ

    Returns:
        pd.DataFrame: ‰ªÖÂåÖÂê´ÊúâÊïà‰ª£ÁêÜÁöÑDataFrameÔºàÁ©∫ÂàôËøîÂõûÁ©∫DataFrameÔºâ
    """
    if proxy_df.empty:
        logging.warning("‚ö†Ô∏è  ÂæÖÈ™åËØÅ‰ª£ÁêÜ‰∏∫Á©∫ÔºåÊó†ÈúÄÈ™åËØÅ")
        return pd.DataFrame()

    logging.info("üìã ÂºÄÂßãÊâπÈáèÈ™åËØÅ‰ª£ÁêÜÔºåÂæÖÈ™åËØÅÊï∞ÈáèÔºö%dÔºåÂπ∂ÂèëÁ∫øÁ®ãÊï∞Ôºö%dÔºåË∂ÖÊó∂Êó∂Èó¥Ôºö%ds",
                 len(proxy_df), max_workers, timeout)

    # Â≠òÂÇ®ÊúâÊïà‰ª£ÁêÜÁöÑÁ¥¢Âºï
    valid_proxy_indices = []

    # Â§öÁ∫øÁ®ãÊâßË°åÈ™åËØÅ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ‰∏∫ÊØè‰∏™‰ª£ÁêÜÊèê‰∫§È™åËØÅ‰ªªÂä°ÔºàËøîÂõû (‰ªªÂä°, ‰ª£ÁêÜÁ¥¢Âºï) Êò†Â∞ÑÔºâ
        task_map = {}
        for idx, row in proxy_df.iterrows():
            ip = str(row["IP Address"]).strip()
            port = int(row["Port"])  # Á°Æ‰øùÁ´ØÂè£‰∏∫Êï¥Êï∞
            task = executor.submit(verify_single_proxy, ip=ip, port=port, timeout=timeout)
            task_map[task] = idx

        # ÈÅçÂéÜÂÆåÊàêÁöÑ‰ªªÂä°ÔºåÊî∂ÈõÜÊúâÊïà‰ª£ÁêÜÁ¥¢Âºï
        for task in as_completed(task_map):
            idx = task_map[task]
            if task.result():  # Ëã•‰ª£ÁêÜÊúâÊïàÔºåËÆ∞ÂΩïÁ¥¢Âºï
                valid_proxy_indices.append(idx)

    # ËøáÊª§ÊúâÊïà‰ª£ÁêÜ
    valid_proxy_df = proxy_df.loc[valid_proxy_indices].reset_index(drop=True)
    logging.info("üìä ‰ª£ÁêÜÈ™åËØÅÂÆåÊàêÔºöÂæÖÈ™åËØÅ%dÊù° ‚Üí ÊúâÊïà%dÊù° ‚Üí Êó†Êïà%dÊù°",
                 len(proxy_df), len(valid_proxy_df), len(proxy_df) - len(valid_proxy_df))

    return valid_proxy_df


# -------------------------- 3. Ê†∏ÂøÉÁà¨ÂèñÂáΩÊï∞ÔºàÊï¥ÂêàÈ™åËØÅ+Êó•ÂøóÔºâ --------------------------
def crawl_free_proxies(
        target_url: str,
        save_dir: str = "./proxy_data",
        filter_invalid: bool = True,
        verify_proxies: bool = True,  # ÊòØÂê¶ÂºÄÂêØ‰ª£ÁêÜÈ™åËØÅ
        max_workers: int = 10,
        verify_timeout: float = 5.0
) -> pd.DataFrame:
    """
    Êó†Â§¥Ê®°ÂºèÁà¨ÂèñÂÖçË¥π‰ª£ÁêÜ ‚Üí ÔºàÂèØÈÄâÔºâËøáÊª§Êó†ÊïàIP ‚Üí ÔºàÂèØÈÄâÔºâÈ™åËØÅÊúâÊïàÊÄß ‚Üí ‰øùÂ≠òCSV ‚Üí ËøîÂõûDataFrame

    Args:
        target_url: ÁõÆÊ†á‰ª£ÁêÜÈ°µÈù¢URL
        save_dir: CSV‰øùÂ≠òÁõÆÂΩï
        filter_invalid: ÊòØÂê¶ËøáÊª§ÂÜÖÁΩë/Êó†ÊïàIPÔºà0.0.0.0„ÄÅ127.0.0.1Á≠âÔºâ
        verify_proxies: ÊòØÂê¶ÂºÄÂêØ‰ª£ÁêÜÊúâÊïàÊÄßÈ™åËØÅÔºàÈªòËÆ§ÂºÄÂêØÔºâ
        max_workers: ‰ª£ÁêÜÈ™åËØÅÊúÄÂ§ßÂπ∂ÂèëÁ∫øÁ®ãÊï∞
        verify_timeout: Âçï‰∏™‰ª£ÁêÜÈ™åËØÅË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ

    Returns:
        pd.DataFrame: ÊúâÊïà‰ª£ÁêÜDataFrameÔºàÂ§±Ë¥•ËøîÂõûNoneÔºâ
    """
    logging.info("üöÄ ÂºÄÂßãÁà¨ÂèñÂÖçË¥π‰ª£ÁêÜÔºåÁõÆÊ†áURLÔºö%s", target_url)

    # 1. ÂàùÂßãÂåñ‰øùÂ≠òÁõÆÂΩï
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        logging.debug("üìÇ ÂàõÂª∫‰ª£ÁêÜÊï∞ÊçÆ‰øùÂ≠òÁõÆÂΩïÔºö%s", save_dir)

    # 2. PlaywrightÊó†Â§¥Áà¨Âèñ
    browser = None
    context = None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,  # Êîπ‰∏∫Êó†Â§¥Ê®°Âºè
                args=["--no-sandbox", "--disable-dev-shm-usage"]  # Ëß£ÂÜ≥LinuxÊùÉÈôêÈóÆÈ¢ò
            )
            context = browser.new_context()
            page = context.new_page()
            logging.debug("üåê ÂêØÂä®Êó†Â§¥ChromiumÊµèËßàÂô®ÔºåËÆøÈóÆÁõÆÊ†áÈ°µÈù¢")

            # ËÆøÈóÆÈ°µÈù¢ÔºàË∂ÖÊó∂30ÁßíÔºâ
            page.goto(target_url, timeout=30000)
            page.wait_for_load_state("networkidle", timeout=20000)  # Á≠âÂæÖÁΩëÁªúÁ©∫Èó≤
            logging.debug("‚úÖ ÁõÆÊ†áÈ°µÈù¢Âä†ËΩΩÂÆåÊàê")

            # 3. ÊèêÂèñ‰ª£ÁêÜÊï∞ÊçÆÔºà‰ΩøÁî®Êñ∞ÁöÑÊñπÊ≥ïÔºâ
            logging.debug("üì• ÂºÄÂßãÊèêÂèñ‰ª£ÁêÜÊï∞ÊçÆ...")
            
            # ‰ΩøÁî®JavaScriptÊèêÂèñ‰ª£ÁêÜÊï∞ÊçÆ
            proxy_data = page.evaluate("""() => {
                const rows = [];
                // Á≠âÂæÖË°®Ê†ºÂä†ËΩΩÂÆåÊàê
                const ipElements = Array.from(document.querySelectorAll('td:nth-child(1)'));
                const portElements = Array.from(document.querySelectorAll('td:nth-child(2)'));
                const codeElements = Array.from(document.querySelectorAll('td:nth-child(3)'));
                const countryElements = Array.from(document.querySelectorAll('td:nth-child(4)'));
                const anonymityElements = Array.from(document.querySelectorAll('td:nth-child(5)'));
                const googleElements = Array.from(document.querySelectorAll('td:nth-child(6)'));
                const httpsElements = Array.from(document.querySelectorAll('td:nth-child(7)'));
                const checkedElements = Array.from(document.querySelectorAll('td:nth-child(8)'));
                
                for (let i = 0; i < Math.min(ipElements.length, portElements.length); i++) {
                    const ip = ipElements[i].textContent.trim();
                    const port = portElements[i] ? portElements[i].textContent.trim() : '';
                    const code = codeElements[i] ? codeElements[i].textContent.trim() : '';
                    const country = countryElements[i] ? countryElements[i].textContent.trim() : '';
                    const anonymity = anonymityElements[i] ? anonymityElements[i].textContent.trim() : '';
                    const google = googleElements[i] ? googleElements[i].textContent.trim() : '';
                    const https = httpsElements[i] ? httpsElements[i].textContent.trim() : '';
                    const checked = checkedElements[i] ? checkedElements[i].textContent.trim() : '';
                    
                    if (ip && port) {
                        rows.push({
                            ip, port, code, country, anonymity, google, https, checked
                        });
                    }
                }
                
                return rows;
            }""")
            
            logging.debug("üì• ÊèêÂèñ‰ª£ÁêÜÊï∞ÊçÆÂÆåÊàêÔºåÂÖ±Ëé∑Âèñ %d Êù°ËÆ∞ÂΩï", len(proxy_data))

            # 4. ËΩ¨Êç¢‰∏∫DataFrame
            if not proxy_data:
                logging.error("‚ùå Êú™Ëé∑ÂèñÂà∞‰ª£ÁêÜÊï∞ÊçÆ")
                return None
                
            df = pd.DataFrame(proxy_data)
            # ÈáçÂëΩÂêçÂàó‰ª•ÂåπÈÖçÂéüÊúâÊ†ºÂºè
            df = df.rename(columns={
                'ip': 'IP Address',
                'port': 'Port',
                'code': 'Code',
                'country': 'Country',
                'anonymity': 'Anonymity',
                'google': 'Google',
                'https': 'Https',
                'checked': 'Last Checked'
            })
            
            logging.info("üìä ‰ª£ÁêÜÊï∞ÊçÆËß£ÊûêÂÆåÊàêÔºåÂÖ±%dÊù°ËÆ∞ÂΩïÔºàÂ≠óÊÆµÔºö%sÔºâ",
                         len(df), ", ".join(df.columns.tolist()))

            # 5. ËøáÊª§ÂÜÖÁΩë/Êó†ÊïàIP
            if filter_invalid:
                invalid_ips = ["0.0.0.0", "127.0.0.1", "localhost"]
                before_filter = len(df)
                df = df[~df["IP Address"].isin(invalid_ips)]
                df = df[df["Port"].apply(lambda x: str(x).isdigit())]  # ËøáÊª§ÈùûÊï∞Â≠óÁ´ØÂè£
                logging.info("üîç ËøáÊª§Êó†ÊïàIP/Á´ØÂè£ÔºöËøáÊª§Ââç%dÊù° ‚Üí ËøáÊª§Âêé%dÊù°", before_filter, len(df))

            # 6. ‰ª£ÁêÜÊúâÊïàÊÄßÈ™åËØÅÔºàÂèØÈÄâÔºâ
            if verify_proxies and not df.empty:
                df = verify_proxy_batch(df, max_workers=max_workers, timeout=verify_timeout)

            # 7. ‰øùÂ≠òCSVÔºàÊåâÊó•ÊúüÂëΩÂêçÔºâ
            today_date = datetime.now().strftime("%Y%m%d")
            save_path = os.path.join(save_dir, f"free_proxies_valid_{today_date}.csv")  # Êñá‰ª∂ÂêçÂä†validÂå∫ÂàÜÊúâÊïà‰ª£ÁêÜ
            df.to_csv(save_path, index=False, encoding="utf-8-sig")
            logging.info("üíæ ÊúâÊïà‰ª£ÁêÜÊï∞ÊçÆ‰øùÂ≠òÂÆåÊàêÔºåË∑ØÂæÑÔºö%sÔºåÊúâÊïàËÆ∞ÂΩïÊï∞Ôºö%d", save_path, len(df))

            return df

    except TimeoutError:
        logging.error("‚ùå Áà¨ÂèñË∂ÖÊó∂ÔºöÈ°µÈù¢Âä†ËΩΩÊàñÂÖÉÁ¥†ÂÆö‰ΩçË∂ÖËøá30ÁßíÔºàÁõÆÊ†áURLÔºö%sÔºâ", target_url)
    except Exception as e:
        logging.error("‚ùå Áà¨ÂèñÂºÇÂ∏∏Ôºö%s", str(e), exc_info=True)  # exc_info=TrueËÆ∞ÂΩïÂÆåÊï¥Â†ÜÊ†à‰ø°ÊÅØ
    finally:
        # ÂÆâÂÖ®ÂÖ≥Èó≠ÊµèËßàÂô®
        try:
            if context:
                context.close()
        except:
            pass
        try:
            if browser:
                browser.close()
        except:
            pass
        logging.debug("üîå ÂÖ≥Èó≠Êó†Â§¥ChromiumÊµèËßàÂô®")

    return None


# -------------------------- 4. ÂÆöÊó∂‰ªªÂä°ÂáΩÊï∞ÔºàÊï¥ÂêàÊó•ÂøóÔºâ --------------------------
def schedule_daily_crawl(
        target_url: str,
        save_dir: str = "./proxy_data",
        crawl_time: str = "02:00",
        verify_proxies: bool = True,
        max_workers: int = 10,
        verify_timeout: float = 5.0
):
    """
    ÊØèÂ§©ÊåáÂÆöÊó∂Èó¥ÂÆöÊó∂Áà¨Âèñ‰ª£ÁêÜÔºàÊï¥ÂêàÊó•ÂøóËÆ∞ÂΩïÔºâ

    Args:
        target_url: ÁõÆÊ†á‰ª£ÁêÜÈ°µÈù¢URL
        save_dir: CSV‰øùÂ≠òÁõÆÂΩï
        crawl_time: ÊØèÂ§©Áà¨ÂèñÊó∂Èó¥ÔºàÊ†ºÂºè"HH:MM"Ôºâ
        verify_proxies: ÊòØÂê¶ÂºÄÂêØ‰ª£ÁêÜÈ™åËØÅ
        max_workers: È™åËØÅÂπ∂ÂèëÁ∫øÁ®ãÊï∞
        verify_timeout: È™åËØÅË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ
    """
    # È¶ñÊ¨°ËøêË°åÁ´ãÂç≥Áà¨Âèñ
    logging.info("üìÖ È¶ñÊ¨°Áà¨ÂèñÂêØÂä®ÔºàÂΩìÂâçÊó∂Èó¥Ôºö%sÔºâ", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    crawl_free_proxies(
        target_url=target_url,
        save_dir=save_dir,
        verify_proxies=verify_proxies,
        max_workers=max_workers,
        verify_timeout=verify_timeout
    )

    # ËÆæÁΩÆÊØèÊó•ÂÆöÊó∂‰ªªÂä°
    schedule.every().day.at(crawl_time).do(
        crawl_free_proxies,
        target_url=target_url,
        save_dir=save_dir,
        verify_proxies=verify_proxies,
        max_workers=max_workers,
        verify_timeout=verify_timeout
    )
    logging.info("‚è∞ ÂÆöÊó∂‰ªªÂä°ÈÖçÁΩÆÂÆåÊàêÔºöÊØèÂ§© %s Ëá™Âä®ÊâßË°å‰ª£ÁêÜÁà¨Âèñ", crawl_time)
    logging.info("‚ÑπÔ∏è  Á®ãÂ∫èËøêË°å‰∏≠ÔºåÊåâ Ctrl+C ÁªàÊ≠¢...")

    # Âæ™ÁéØÁõëÂê¨‰ªªÂä°ÔºàÊØèÂàÜÈíüÊ£ÄÊü•‰∏ÄÊ¨°Ôºâ
    while True:
        schedule.run_pending()
        time.sleep(60)


# -------------------------- Á®ãÂ∫èÂÖ•Âè£ÔºàÂàùÂßãÂåñÊó•Âøó+ÂêØÂä®ÂÆöÊó∂‰ªªÂä°Ôºâ --------------------------
if __name__ == "__main__":
    # -------------------------- ÈÖçÁΩÆÂèÇÊï∞ÔºàËØ∑Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµ‰øÆÊîπÔºâ --------------------------
    TARGET_URL = "https://free-proxy-list.net"  # ÁõÆÊ†á‰ª£ÁêÜÈ°µÈù¢ÁúüÂÆûURLÔºàÁ§∫‰æãÔºöfree-proxy-list.netÔºâ
    SAVE_DIR = "./proxy_data"  # ÊúâÊïà‰ª£ÁêÜCSV‰øùÂ≠òÁõÆÂΩï
    LOG_DIR = "./proxy_logs"  # Êó•ÂøóÊñá‰ª∂‰øùÂ≠òÁõÆÂΩï
    DAILY_CRAWL_TIME = "02:00"  # ÊØèÊó•Áà¨ÂèñÊó∂Èó¥Ôºà24Â∞èÊó∂Âà∂ÔºåÂ¶Ç"02:00"Ôºâ
    VERIFY_PROXIES = True  # ÊòØÂê¶ÂºÄÂêØ‰ª£ÁêÜÊúâÊïàÊÄßÈ™åËØÅ
    MAX_WORKERS = 15  # ‰ª£ÁêÜÈ™åËØÅÊúÄÂ§ßÂπ∂ÂèëÁ∫øÁ®ãÊï∞ÔºàÂª∫ËÆÆ10-20Ôºâ
    VERIFY_TIMEOUT = 6.0  # Âçï‰∏™‰ª£ÁêÜÈ™åËØÅË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºåÂª∫ËÆÆ5-10Ôºâ
    # ----------------------------------------------------------------------------------

    # 1. ÂàùÂßãÂåñÊó•ÂøóÁ≥ªÁªüÔºàÂøÖÈ°ªÂú®ÊúÄÂâçÈù¢ÊâßË°åÔºåÁ°Æ‰øùÂêéÁª≠ÊµÅÁ®ãÊó•ÂøóÊ≠£Â∏∏ËÆ∞ÂΩïÔºâ
    init_logger(log_dir=LOG_DIR)
    
    # 2. Áà¨Âèñ‰ª£ÁêÜ
    crawl_free_proxies(target_url=TARGET_URL, save_dir=SAVE_DIR, verify_proxies=VERIFY_PROXIES, 
                      max_workers=MAX_WORKERS, verify_timeout=VERIFY_TIMEOUT)

    # # 3. ÂêØÂä®ÂÆöÊó∂Áà¨Âèñ‰ªªÂä°
    # try:
    #     schedule_daily_crawl(
    #         target_url=TARGET_URL,
    #         save_dir=SAVE_DIR,
    #         crawl_time=DAILY_CRAWL_TIME,
    #         verify_proxies=VERIFY_PROXIES,
    #         max_workers=MAX_WORKERS,
    #         verify_timeout=VERIFY_TIMEOUT
    #     )
    # except KeyboardInterrupt:
    #     logging.info("üõë Áî®Êà∑ÊâãÂä®ÁªàÊ≠¢Á®ãÂ∫èÔºàCtrl+CÔºâÔºåÁ®ãÂ∫èÈÄÄÂá∫")
    # except Exception as e:
    #     logging.critical("üí• Á®ãÂ∫èÊÑèÂ§ñÁªàÊ≠¢ÔºåÂºÇÂ∏∏‰ø°ÊÅØÔºö%s", str(e), exc_info=True)