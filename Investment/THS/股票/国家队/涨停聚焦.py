from pprint import pprint

import pandas as pd
import requests
import json
from urllib.parse import urlencode


def fetch_limit_up_data(type_key, date, page=1, limit=5):
    """
    å‘é€GETè¯·æ±‚è·å–è‚¡ç¥¨æ¶¨åœæ± æ•°æ®

    å‚æ•°:
        page (int): é¡µç ï¼Œé»˜è®¤ä¸º1
        limit (int): æ¯é¡µæ•°æ®æ¡æ•°ï¼Œé»˜è®¤ä¸º15

    è¿”å›:
        dict: è¯·æ±‚è¿”å›çš„JSONæ•°æ®

    å¼‚å¸¸:
        å¯èƒ½æŠ›å‡ºrequestsåº“çš„å¼‚å¸¸
    """
    # åŸºç¡€URL
    base_url = f"https://data.10jqka.com.cn/dataapi/limit_up/{type_key}"

    # è¯·æ±‚å‚æ•°
    params = {
        "page": page,
        "limit": limit,
        "field": "199112,10,9001,330323,330324,330325,9002,330329,133971,133970,1968584,3475914,9003,9004",
        "filter": "HS,GEM2STAR",
        "order_field": "330324",#119112ä¸ºå†²åˆºæ¶¨åœï¼Œ330329ä¸ºè¿æ¿æ± ï¼Œ
        "order_type": "0",
        "date": date,
        # "_": "1749898676743"
    }

    # è¯·æ±‚å¤´ä¿¡æ¯
    headers = {
        "sec-ch-ua": '"Chromium";v="116", "Not)A;Brand";v="24", "Android WebView";v="116"',
        "Accept": "application/json, text/plain, */*",
        "User-Agent": "Mozilla/5.0 (Linux; Android 14; V2353A Build/UP1A.231005.007; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/116.0.0.0 Mobile Safari/537.36 Hexin_Gphone/11.30.02 (Royal Flush) hxtheme/1 innerversion/G037.09.033.1.32 followPhoneSystemTheme/1 userid/641926488 getHXAPPAccessibilityMode/0 hxNewFont/1 isVip/0 getHXAPPFontSetting/normal getHXAPPAdaptOldSetting/0",
        "sec-ch-ua-platform": '"Android"',
        "X-Requested-With": "com.hexin.plat.android",
        "Referer": "https://data.10jqka.com.cn/datacenterph/limitup/limtupInfo.html",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Cookie": "user_status=0; user=MDptb182NDE5MjY0ODg6Ok5vbmU6NTAwOjY1MTkyNjQ4ODo3LDExMTExMTExMTExLDQwOzQ0LDExLDQwOzYsMSw0MDs1LDEsNDA7MSwxMDEsNDA7MiwxLDQwOzMsMSw0MDs1LDEsNDA7OCwwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMSw0MDsxMDIsMSw0MDoyNzo6OjY0MTkyNjQ4ODoxNzQ5NjkzMjg5Ojo6MTY1ODE0Mjc4MDoyNjc4NDAwOjA6MTVjNGY3MWViY2M0YmQwNDBkNGU1MDEzYzdmM2Q0NWRmOjox; userid=641926488; u_name=mo_641926488; escapename=mo_641926488; ticket=536749b3c84105bd1c392b267cb5d589; IFUserCookieKey={\"userid\":\"641926488\",\"escapename\":\"mo_641926488\",\"custid\":\"\"); _clck=a5x9j2%7C2%7Cfwp%7C0%7C0; hxmPid=free_ztjj; v=AwCzQhCrfI5n3ACKfzWWz20Y04XSieRThm04V3qRzJuu9a-_Ipm049Z9COPJ"
    }

    try:
        # å‘é€GETè¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†å‚æ•°ç¼–ç 
        response = requests.get(base_url, params=params, headers=headers, timeout=15)

        # æ£€æŸ¥å“åº”çŠ¶æ€ç 
        if response.status_code == 200:
            # å°è¯•è§£æJSONå“åº”
            try:
                return response.json()
            except json.JSONDecodeError:
                print("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                return {"error": "Invalid JSON response", "text": response.text}
        else:
            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            return {"error": f"Request failed with status code {response.status_code}"}

    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {e}")
        return {"error": f"Request exception: {str(e)}"}

def extract_limit_up_pool(json_data):
    """
    æå–ã€æ¶¨åœå¼ºåº¦ã€‘ä¸­çš„è‚¡ç¥¨ä¿¡æ¯
    """
    stock_list = json_data.get('data', {}).get('info', [])
    extracted_data = []

    for item in stock_list:
        extracted_data.append({
            'è‚¡ç¥¨ä»£ç ': item['code'],
            'è‚¡ç¥¨åç§°': item['name'],
            'æ¶¨è·Œå¹…(%)': round(item['change_rate'], 2),
            'æ¶¨åœåŸå› ': item.get('reason_type', ''),
            'æœ€æ–°ä»·': item['latest'],
            'æ¶¨åœç±»å‹': item.get('limit_up_type', ''),
            'æ˜¯å¦é¦–æ¿': 'æ˜¯' if item['high_days'] == 'é¦–æ¿' else 'å¦',
            'æ¢æ‰‹ç‡(%)': item.get('turnover_rate', 0)
        })

    return extracted_data


def extract_strongest_block(json_data):
    """
    æå–ã€æœ€å¼ºé£å£ã€‘ä¸­çš„è‚¡ç¥¨ä¿¡æ¯
    """
    blocks = json_data.get('data', [])
    extracted_data = []

    for block in blocks:
        block_name = block['name']
        stock_list = block.get('stock_list', [])

        for item in stock_list:
            extracted_data.append({
                'æ‰€å±æ¿å—': block_name,
                'è‚¡ç¥¨ä»£ç ': item['code'],
                'è‚¡ç¥¨åç§°': item['name'],
                'æ¶¨è·Œå¹…(%)': round(item['change_rate'], 2),
                'æ¶¨åœåŸå› ': item.get('reason_type', ''),
                'æœ€æ–°ä»·': item['latest'],
                'æ˜¯å¦é¦–æ¿': 'æ˜¯' if item['high'] == 'é¦–æ¿' else 'å¦'
            })

    return extracted_data


def extract_continuous_limit_up(json_data):
    """
    æå–ã€è¿æ¿å¤©æ¢¯ã€‘ä¸­çš„è‚¡ç¥¨ä¿¡æ¯
    """
    heights = json_data.get('data', [])
    extracted_data = []

    for height_info in heights:
        height = height_info['height']
        stocks = height_info.get('code_list', [])

        for item in stocks:
            extracted_data.append({
                'æ¶¨åœé«˜åº¦': height,
                'è‚¡ç¥¨ä»£ç ': item['code'],
                'è‚¡ç¥¨åç§°': item['name'],
                'è¿ç»­æ¶¨åœå¤©æ•°': item['continue_num']
            })

    return extracted_data

# è°ƒç”¨å‡½æ•°è·å–ç¬¬ä¸€é¡µæ•°æ®
if __name__ == "__main__":
    date = "20240613"
    types = {
        'limit_up_pool': 'æ¶¨åœå¼ºåº¦',
        'block_top': 'æœ€å¼ºé£å£',
        'continuous_limit_up': 'è¿æ¿å¤©æ¢¯',
    }

    all_dfs = {}

    for type_key, type_name in types.items():
        print(f"æ­£åœ¨è·å– {type_name} æ•°æ®...")
        data = fetch_limit_up_data(type_key,date)
        pprint(data)

        if type_key == 'limit_up_pool':
            df = pd.DataFrame(extract_limit_up_pool(data))
        elif type_key == 'block_top':
            df = pd.DataFrame(extract_strongest_block(data))
        elif type_key == 'continuous_limit_up':
            df = pd.DataFrame(extract_continuous_limit_up(data))

        # æ§åˆ¶å°æ‰“å°
        print(f"\nğŸ“Š {type_name} æ•°æ®è¡¨ï¼š")
        print(df)

        # å­˜å…¥å­—å…¸ï¼Œä¾¿äºåç»­å†™å…¥å¤šä¸ªsheet
        all_dfs[type_name] = df

    # å†™å…¥ Excel
    output_file = 'æ¶¨åœç»¼åˆæ•°æ®.xlsx'

    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        for sheet_name, df in all_dfs.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)

    print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
