#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç»œçˆ¬è™«æ¨¡å— - ç”¨äºä»äº’è”ç½‘ä¸ŠæŠ“å–è§†é¢‘å’Œå›¾ç‰‡ç´ æ
æ³¨æ„ï¼šæ­¤æ¨¡å—ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·éµå®ˆå„ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾å’Œç‰ˆæƒè§„å®š
"""

import requests
import os
import time
import random
from typing import List, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup


class WebScraper:
    """ç½‘ç»œçˆ¬è™«ç±»"""
    
    def __init__(self, save_dir: str = "./scraped_materials"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param save_dir: ä¿å­˜ç›®å½•
        """
        self.save_dir = save_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(save_dir, "videos"), exist_ok=True)
    
    def _download_file(self, url: str, save_path: str) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶
        :param url: æ–‡ä»¶URL
        :param save_path: ä¿å­˜è·¯å¾„
        :return: æ˜¯å¦æˆåŠŸ
        """
        try:
            response = self.session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(save_path)}")
            return True
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ {url}: {str(e)}")
            return False
    
    def search_free_images(self, keywords: str, max_results: int = 10) -> List[str]:
        """
        ä»å…è´¹å›¾åº“æœç´¢å›¾ç‰‡ï¼ˆç¤ºä¾‹å®ç°ï¼‰
        :param keywords: æœç´¢å…³é”®è¯
        :param max_results: æœ€å¤§ç»“æœæ•°
        :return: å›¾ç‰‡URLåˆ—è¡¨
        """
        # è¿™é‡Œä»…ä½œç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´
        image_urls = []
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        print(f"ğŸ” æœç´¢å›¾ç‰‡å…³é”®è¯: {keywords}")
        
        # ç¤ºä¾‹URLï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦ä»çœŸå®ç½‘ç«™æŠ“å–ï¼‰
        sample_urls = [
            "https://picsum.photos/1920/1080",
            "https://picsum.photos/1280/720",
            "https://picsum.photos/800/600"
        ]
        
        for i in range(min(max_results, len(sample_urls))):
            # ä½¿ç”¨Lorem Picsumç”Ÿæˆéšæœºå›¾ç‰‡
            image_urls.append(f"{sample_urls[i%len(sample_urls)]}?random={random.randint(1, 1000)}")
            time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return image_urls
    
    def download_images(self, urls: List[str], prefix: str = "scraped") -> List[str]:
        """
        ä¸‹è½½å›¾ç‰‡
        :param urls: å›¾ç‰‡URLåˆ—è¡¨
        :param prefix: æ–‡ä»¶åå‰ç¼€
        :return: æˆåŠŸä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        saved_paths = []
        images_dir = os.path.join(self.save_dir, "images")
        
        for i, url in enumerate(urls):
            try:
                # ç”Ÿæˆæ–‡ä»¶å
                parsed_url = urlparse(url)
                ext = os.path.splitext(parsed_url.path)[1] or ".jpg"
                filename = f"{prefix}_{i+1:03d}{ext}"
                save_path = os.path.join(images_dir, filename)
                
                # ä¸‹è½½æ–‡ä»¶
                if self._download_file(url, save_path):
                    saved_paths.append(save_path)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ {url}: {str(e)}")
        
        return saved_paths
    
    def search_video_sites(self, keywords: str) -> List[str]:
        """
        æœç´¢è§†é¢‘ç½‘ç«™ï¼ˆä»…è¿”å›ç¤ºä¾‹æ•°æ®ï¼‰
        :param keywords: æœç´¢å…³é”®è¯
        :return: è§†é¢‘é¡µé¢URLåˆ—è¡¨
        """
        print(f"ğŸ” æœç´¢è§†é¢‘å…³é”®è¯: {keywords}")
        
        # ç¤ºä¾‹è§†é¢‘ç½‘ç«™ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦è§£æç½‘ç«™å†…å®¹ï¼‰
        video_sites = [
            "https://www.videvo.net/",
            "https://www.videezy.com/",
            "https://coverr.co/",
            "https://mixkit.co/"
        ]
        
        return video_sites
    
    def get_trending_videos(self, site_url: str, category: str = "all") -> List[dict]:
        """
        è·å–çƒ­é—¨è§†é¢‘ï¼ˆç¤ºä¾‹å®ç°ï¼‰
        :param site_url: ç½‘ç«™URL
        :param category: åˆ†ç±»
        :return: è§†é¢‘ä¿¡æ¯åˆ—è¡¨
        """
        print(f"ğŸ“ˆ è·å– {site_url} çš„çƒ­é—¨è§†é¢‘...")
        
        # ç¤ºä¾‹æ•°æ®
        sample_videos = [
            {
                "title": "Nature Landscape",
                "url": "https://sample-videos.com/zip.php?file=video1.zip",
                "duration": "0:30",
                "resolution": "1920x1080"
            },
            {
                "title": "City Time-lapse",
                "url": "https://sample-videos.com/zip.php?file=video2.zip",
                "duration": "0:45",
                "resolution": "1280x720"
            }
        ]
        
        return sample_videos
    
    def download_video(self, video_info: dict, prefix: str = "scraped") -> Optional[str]:
        """
        ä¸‹è½½è§†é¢‘ï¼ˆç¤ºä¾‹å®ç°ï¼‰
        :param video_info: è§†é¢‘ä¿¡æ¯
        :param prefix: æ–‡ä»¶åå‰ç¼€
        :return: ä¿å­˜è·¯å¾„æˆ–None
        """
        try:
            video_url = video_info.get("url")
            if not video_url:
                return None
            
            videos_dir = os.path.join(self.save_dir, "videos")
            title = video_info.get("title", "untitled")
            filename = f"{prefix}_{title.replace(' ', '_')}.mp4"
            save_path = os.path.join(videos_dir, filename)
            
            if self._download_file(video_url, save_path):
                return save_path
        except Exception as e:
            print(f"âŒ ä¸‹è½½è§†é¢‘å¤±è´¥: {str(e)}")
        
        return None


def integrate_with_jianying(materials_dir: str = "./scraped_materials") -> dict:
    """
    ä¸å‰ªæ˜ é›†æˆ
    :param materials_dir: ç´ æç›®å½•
    :return: å¯ç”¨äºå‰ªæ˜ çš„ç´ æä¿¡æ¯
    """
    images_dir = os.path.join(materials_dir, "images")
    videos_dir = os.path.join(materials_dir, "videos")
    
    materials = {
        "images": [],
        "videos": []
    }
    
    # æ”¶é›†å›¾ç‰‡
    if os.path.exists(images_dir):
        for file in os.listdir(images_dir):
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                materials["images"].append(os.path.join(images_dir, file))
    
    # æ”¶é›†è§†é¢‘
    if os.path.exists(videos_dir):
        for file in os.listdir(videos_dir):
            if file.lower().endswith(('.mp4', '.mov', '.avi', '.mkv')):
                materials["videos"].append(os.path.join(videos_dir, file))
    
    return materials


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = WebScraper("./downloaded_materials")
    
    # æœç´¢å¹¶ä¸‹è½½å›¾ç‰‡
    print("å¼€å§‹ä¸‹è½½å›¾ç‰‡ç´ æ...")
    image_urls = scraper.search_free_images("nature landscape", max_results=5)
    saved_images = scraper.download_images(image_urls, "nature")
    print(f"å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len(saved_images)} å¼ ")
    
    # æœç´¢è§†é¢‘ç½‘ç«™
    print("\næœç´¢è§†é¢‘ç´ ææ¥æº...")
    video_sites = scraper.search_video_sites("music video")
    print(f"æ‰¾åˆ°è§†é¢‘ç½‘ç«™: {len(video_sites)} ä¸ª")
    
    # è·å–çƒ­é—¨è§†é¢‘
    if video_sites:
        trending_videos = scraper.get_trending_videos(video_sites[0])
        print(f"è·å–åˆ° {len(trending_videos)} ä¸ªçƒ­é—¨è§†é¢‘")
    
    # ä¸å‰ªæ˜ é›†æˆ
    print("\næ•´åˆç´ æç”¨äºå‰ªæ˜ ...")
    materials = integrate_with_jianying("./downloaded_materials")
    print(f"å¯ç”¨äºå‰ªæ˜ çš„ç´ æ: {len(materials['images'])} å¼ å›¾ç‰‡, {len(materials['videos'])} ä¸ªè§†é¢‘")